# -*- coding: utf-8 -*-
"""flight fare predicton.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yVKwROYqHy_4XASa64f94L2bA0ozCMKK

<a href="https://colab.research.google.com/github/goutam63/machine-learning-projects/blob/main/flight_fare_predicton.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

train_data=pd.read_excel('Data_Train.xlsx')

"""1.this is the excel file so we have to use the pandas read_excel insted of read_csv

2.after importing and loading the file and all libraries we have to check for the null data in file. if there is null data in columns we have the do the follosing steps.

1.   impute the data using the imputation method 
2.   filling the values with mean,median,mode using the fillna() method of pandsa 

- 
"""

train_data.head()

""" # the info() method shows the number of rows and columns the dataset contain and which type data it has in its perticular columns."""

train_data.info()

train_data.isnull().sum()

"""

*   very less data is missing in dataset as we can drop the missing data from the dataset usnig the dropna method.
"""

train_data.dropna(inplace=True)

train_data.isnull().sum()

"""# EDA 
from description we can see that fligh fare dataset is having the datatime object datatype that is Date_of_Joutney. we have convert this columns
into timestamp use it in prediction

for this we use <strong>to_datatime</strong> method to convert object datatype to datatime datatype 

**.dt.day method extract the only day from date **\ **.dt.month it extract the only month from that date**\

"""

train_data['jurney_day']=pd.to_datetime(train_data.Date_of_Journey,format="%d/%m/%Y").dt.day

train_data['jurney_month']=pd.to_datetime(train_data.Date_of_Journey,format="%d/%m/%Y").dt.month

train_data.head()

# since we have converted the Date_of_jurney columns into day and month so we can drop the columns 
train_data.drop('Date_of_Journey',axis=1,inplace=True)

# departure is the time whe the flight leaved the gate
# we can extract the hour and minute from the Dep_time as in the Date_of_jurney
# Extracting the hour
train_data['hour']=pd.to_datetime(train_data['Dep_Time']).dt.hour
# Extracting the minute 
train_data['minute']=pd.to_datetime(train_data['Dep_Time']).dt.minute
# we eleminate the column Dep_time there no use of it now as we extracted the hour and minute 
train_data.drop('Dep_Time',axis=1,inplace=True)

train_data.head()

# Arrival time is when the plane reaches the gate.
# Similar to Date_of_Journey we can extract values from Arrival_Time

# Extracting Hours
train_data["Arrival_hour"] = pd.to_datetime(train_data.Arrival_Time).dt.hour

# Extracting Minutes
train_data["Arrival_min"] = pd.to_datetime(train_data.Arrival_Time).dt.minute

# Now we can drop Arrival_Time as it is of no use
train_data.drop(["Arrival_Time"], axis = 1, inplace = True)

train_data.head()

# It is the differnce betwwen Departure Time and Arrival time

# Assigning and converting Duration column into list
duration = list(train_data["Duration"])

for i in range(len(duration)):
    if len(duration[i].split()) != 2:    # Check if duration contains only hour or mins
        if "h" in duration[i]:
            duration[i] = duration[i].strip() + " 0m"   # Adds 0 minute
        else:
            duration[i] = "0h " + duration[i]           # Adds 0 hour

duration_hours = []
duration_mins = []
for i in range(len(duration)):
    duration_hours.append(int(duration[i].split(sep = "h")[0]))    # Extract hours from duration
    duration_mins.append(int(duration[i].split(sep = "m")[0].split()[-1]))   # Extracts only minutes from duration

# Adding duration_hours and duration_mins list to train_data dataframe

train_data["Duration_hours"] = duration_hours
train_data["Duration_mins"] = duration_mins

train_data.drop(["Duration"], axis = 1, inplace = True)

train_data.head()

"""# Handling the categorical variables 
 there is many way to handle the categorical data some of them are 

1.   **.Nominal data**--> data are not in any oreder-->**OneHotEncoder** is used
2.   **.Nominal data**--> data are in oreder-->**LabelEncoder** is used


"""

train_data['Airline'].value_counts()

# From graph we can see that Jet Airways Business have the highest Price.
# Apart from the first Airline almost all are having similar median

# Airline vs Price
sns.catplot(y = "Price", x = "Airline", data = train_data.sort_values("Price", ascending = False), kind="boxen", height = 6, aspect = 3)
plt.show()

# As Airline is Nominal Categorical data we will perform OneHotEncoding

Airline = train_data[["Airline"]]

Airline = pd.get_dummies(Airline, drop_first= True)

Airline.head()

train_data["Source"].value_counts()

# Source vs Price

sns.catplot(y = "Price", x = "Source", data = train_data.sort_values("Price", ascending = False), kind="boxen", height = 4, aspect = 3)
plt.show()

# As Source is Nominal Categorical data we will perform OneHotEncoding

Source = train_data[["Source"]]

Source = pd.get_dummies(Source, drop_first= True)

Source.head()

train_data["Destination"].value_counts()

# As Destination is Nominal Categorical data we will perform OneHotEncoding

Destination = train_data[["Destination"]]

Destination = pd.get_dummies(Destination, drop_first = True)

Destination.head()

train_data["Route"]

# Additional_Info contains almost 80% no_info
# Route and Total_Stops are related to each other

#train_data.drop(["Route", "Additional_Info"], axis = 1, inplace = True)
routes = train_data['Route'].str.replace(' â†’ ', ',').str.split(',')
stops = [route[1:-1] if len(route) > 2 else None for route in routes]

train_data["Total_Stops"].value_counts()

# As this is case of Ordinal Categorical type we perform LabelEncoder
# Here Values are assigned with corresponding keys

train_data.replace({"non-stop": 0, "1 stop": 1, "2 stops": 2, "3 stops": 3, "4 stops": 4}, inplace = True)

#Concatenate dataframe --> train_data + Airline + Source + Destination

data_train = pd.concat([train_data, Airline, Source, Destination], axis = 1)

data_train.head()

data_train.drop(["Airline", "Source", "Destination"], axis = 1, inplace = True)

data_train.head()

data_train.shape

"""# Test data

# checking the skeweness of the data

## in this data set only price columns is the numeric column we only check the skewness of the only the numarical data
"""

from scipy.stats import skew
data_train['Price'].skew()
data_train['Price']=np.log10(data_train['Price'])



test_data = pd.read_excel("Test_set.xlsx")

test_data.head()

# Preprocessing

print("Test data Info")
print("-"*75)
print(test_data.info())

print()
print()

print("Null values :")
print("-"*75)
test_data.dropna(inplace = True)
print(test_data.isnull().sum())

# EDA

# Date_of_Journey
test_data["Journey_day"] = pd.to_datetime(test_data.Date_of_Journey, format="%d/%m/%Y").dt.day
test_data["Journey_month"] = pd.to_datetime(test_data["Date_of_Journey"], format = "%d/%m/%Y").dt.month
test_data.drop(["Date_of_Journey"], axis = 1, inplace = True)

# Dep_Time
test_data["Dep_hour"] = pd.to_datetime(test_data["Dep_Time"]).dt.hour
test_data["Dep_min"] = pd.to_datetime(test_data["Dep_Time"]).dt.minute
test_data.drop(["Dep_Time"], axis = 1, inplace = True)

# Arrival_Time
test_data["Arrival_hour"] = pd.to_datetime(test_data.Arrival_Time).dt.hour
test_data["Arrival_min"] = pd.to_datetime(test_data.Arrival_Time).dt.minute
test_data.drop(["Arrival_Time"], axis = 1, inplace = True)

# Duration
duration = list(test_data["Duration"])

for i in range(len(duration)):
    if len(duration[i].split()) != 2:    # Check if duration contains only hour or mins
        if "h" in duration[i]:
            duration[i] = duration[i].strip() + " 0m"   # Adds 0 minute
        else:
            duration[i] = "0h " + duration[i]           # Adds 0 hour

duration_hours = []
duration_mins = []
for i in range(len(duration)):
    duration_hours.append(int(duration[i].split(sep = "h")[0]))    # Extract hours from duration
    duration_mins.append(int(duration[i].split(sep = "m")[0].split()[-1]))   # Extracts only minutes from duration

# Adding Duration column to test set
test_data["Duration_hours"] = duration_hours
test_data["Duration_mins"] = duration_mins
test_data.drop(["Duration"], axis = 1, inplace = True)


# Categorical data

print("Airline")
print("-"*75)
print(test_data["Airline"].value_counts())
Airline = pd.get_dummies(test_data["Airline"], drop_first= True)

print()

print("Source")
print("-"*75)
print(test_data["Source"].value_counts())
Source = pd.get_dummies(test_data["Source"], drop_first= True)

print()

print("Destination")
print("-"*75)
print(test_data["Destination"].value_counts())
Destination = pd.get_dummies(test_data["Destination"], drop_first = True)

# Additional_Info contains almost 80% no_info
# Route and Total_Stops are related to each other
test_data.drop(["Route", "Additional_Info"], axis = 1, inplace = True)

# Replacing Total_Stops
test_data.replace({"non-stop": 0, "1 stop": 1, "2 stops": 2, "3 stops": 3, "4 stops": 4}, inplace = True)

# Concatenate dataframe --> test_data + Airline + Source + Destination
data_test = pd.concat([test_data, Airline, Source, Destination], axis = 1)

data_test.drop(["Airline", "Source", "Destination"], axis = 1, inplace = True)

print()

print()

print("Shape of test data : ", data_test.shape)

data_test.head()

"""# feature selection 
 Finding the best feature which has the best relation with the target variable.following are the some of the feature selection methods 


1.   **heatmap**
2.   **feature importance**
3.   **SelectKBest**


"""

data_train.columns

X = data_train.loc[:, ['Total_Stops','jurney_day', 'jurney_month', 'hour', 'minute',
       'Arrival_hour', 'Arrival_min', 'Duration_hours', 'Duration_mins',
       'Airline_Air India', 'Airline_GoAir', 'Airline_IndiGo',
       'Airline_Jet Airways', 'Airline_Jet Airways Business',
       'Airline_Multiple carriers',
       'Airline_Multiple carriers Premium economy', 'Airline_SpiceJet',
       'Airline_Trujet', 'Airline_Vistara', 'Airline_Vistara Premium economy',
       'Source_Chennai', 'Source_Delhi', 'Source_Kolkata', 'Source_Mumbai',
       'Destination_Cochin', 'Destination_Delhi', 'Destination_Hyderabad',
       'Destination_Kolkata', 'Destination_New Delhi'],]
X.head()

y = data_train.iloc[:, 1]
y.head()

# Finds correlation between Independent and dependent attributes

plt.figure(figsize = (20,10))
sns.heatmap(train_data.corr(), annot = True, cmap = "RdYlGn")

plt.show()

#Important feature using ExtraTreesRegressor

from sklearn.ensemble import ExtraTreesRegressor
selection = ExtraTreesRegressor()
selection.fit(X,y)

print(selection.feature_importances_)

#plot graph of feature importances for better visualization

plt.figure(figsize = (12,8))
feat_importances = pd.Series(selection.feature_importances_, index=X.columns)
feat_importances.nlargest(20).plot(kind='barh')
plt.show()

x = data_train.loc[:, ['Total_Stops', 'Duration_hours',]]
x.head()

"""# Fitting model using Random Forest
 
1.   Split dataset into train and test set in order to prediction w.r.t X_test
2.  If needed do scaling of data
*               Scaling is not done in Random forest
3.   Import model
4.   Fit the data
5.   Predict w.r.t X_test
6.   In regression check RSME Score
7.   Plot graph

"""

#from imblearn.over_sampling import RandomOverSampler
#sample=RandomOverSampler()
#X_train_res,y_train_res=sample.fit_sample(X,y)

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(x,y,test_size = 0.2, random_state = 42)

"""# handling the imblance dataset """

#from imblearn.over_sampling import RandomOverSampler
#sample=RandomOverSampler()
#X_train_res,y_train_res=sample.fit_sample(X,y)

from sklearn.ensemble import RandomForestRegressor
reg_rf = RandomForestRegressor()
reg_rf.fit(X_train, y_train)

y_pred = reg_rf.predict(X_test)

reg_rf.score(X_train, y_train)

reg_rf.score(X_test, y_test)

from sklearn.model_selection import cross_val_score
score=cross_val_score(RandomForestRegressor(),X_train,y_train,cv=5)

score.mean()

from sklearn.model_selection import cross_val_score
score1=cross_val_score(RandomForestRegressor(),X_test,y_test,cv=5)

score1.mean()

sns.distplot(y_test-y_pred)
plt.show()

plt.scatter(y_test, y_pred, alpha = 0.5)
plt.xlabel("y_test")
plt.ylabel("y_pred")
plt.show()

from sklearn import metrics
print('MAE:', metrics.mean_absolute_error(y_test, y_pred))
print('MSE:', metrics.mean_squared_error(y_test, y_pred))
print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))

metrics.r2_score(y_test, y_pred)

1 - (1-reg_rf.score(X_train, y_train))*(len(y_train)-1)/(len(y_train)-X_train.shape[1]-1)

"""# Hyperparameter Tuning
Choose following method for hyperparameter tuning

RandomizedSearchCV --> Fast

GridSearchCV

Assign hyperparameters in form of dictionery

Fit the model

Check best paramters and best score
"""

from sklearn.model_selection import RandomizedSearchCV

#Randomized Search CV

# Number of trees in random forest
n_estimators = [int(x) for x in np.linspace(start = 100, stop = 1200, num = 12)]
# Number of features to consider at every split
max_features = ['auto', 'sqrt']
# Maximum number of levels in tree
max_depth = [int(x) for x in np.linspace(5, 30, num = 6)]
# Minimum number of samples required to split a node
min_samples_split = [2, 5, 10, 15, 100]
# Minimum number of samples required at each leaf node
min_samples_leaf = [1, 2, 5, 10]

# Create the random grid

random_grid = {'n_estimators': n_estimators,
               'max_features': max_features,
               'max_depth': max_depth,
               'min_samples_split': min_samples_split,
               'min_samples_leaf': min_samples_leaf}

# Random search of parameters, using 5 fold cross validation, 
# search across 100 different combinations
rf_random = RandomizedSearchCV(estimator = reg_rf, param_distributions = random_grid,scoring='neg_mean_squared_error', n_iter = 10, cv = 5, verbose=2, random_state=42, n_jobs = 1)

rf_random.fit(X_train,y_train)

print(rf_random.best_params_)
print(rf_random.best_score_)
rf_random.best_score_

prediction = rf_random.predict(X_test)

plt.figure(figsize = (8,8))
sns.distplot(y_test-prediction)
plt.show()

plt.figure(figsize = (8,8))
plt.scatter(y_test, prediction, alpha = 0.5)
plt.xlabel("y_test")
plt.ylabel("y_pred")
plt.show()

print('MAE:', metrics.mean_absolute_error(y_test, prediction))
print('MSE:', metrics.mean_squared_error(y_test, prediction))
print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, prediction)))

"""# Save the model to reuse it again"""

import pickle
# open a file, where you ant to store the data
file = open('flight_rf.pkl', 'wb')

# dump information to that file
pickle.dump(rf_random, file)

model = open('flight_rf.pkl','rb')
forest = pickle.load(model)

y_prediction = forest.predict(X_test)

metrics.r2_score(y_test, y_prediction)